{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOUgMCChjwY9zhVFC7XXHJn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomainD-prog/ProjetParcours/blob/main/DQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Deep QLearing"
      ],
      "metadata": {
        "id": "G3AsoYgLWTRe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation des dépendances"
      ],
      "metadata": {
        "id": "gEN4XtNDYf_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Le code est inspiré de la documentation KERAS sur le breakout : https://keras.io/examples/rl/deep_q_network_breakout/"
      ],
      "metadata": {
        "id": "lp1zfF_LLmjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmzDdKE3IBIF",
        "outputId": "3f216060-fa67-4601-921c-3de4d1858d8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable_baselines\n",
            "  Downloading stable_baselines-2.10.2-py3-none-any.whl (240 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.9/240.9 KB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (1.3.5)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (2.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (1.2.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (1.7.3)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (3.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from stable_baselines) (4.6.0.66)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[atari,classic_control]>=0.11->stable_baselines) (6.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[atari,classic_control]>=0.11->stable_baselines) (0.0.8)\n",
            "Collecting ale-py~=0.7.5\n",
            "  Downloading ale_py-0.7.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines) (1.4.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->stable_baselines) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->stable_baselines) (2022.7)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.8/dist-packages (from ale-py~=0.7.5->gym[atari,classic_control]>=0.11->stable_baselines) (5.10.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[atari,classic_control]>=0.11->stable_baselines) (3.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->stable_baselines) (1.15.0)\n",
            "Installing collected packages: pygame, ale-py, stable_baselines\n",
            "Successfully installed ale-py-0.7.5 pygame-2.1.0 stable_baselines-2.10.2\n"
          ]
        }
      ],
      "source": [
        "!pip install stable_baselines\n",
        "#Download and install ROMs\n",
        "!wget -q http://www.atarimania.com/roms/Roms.rar\n",
        "!pip install -q unrar\n",
        "!mkdir ./roms_atari\n",
        "!unrar x Roms.rar ./roms_atari > /dev/null 2>&1\n",
        "!python -m atari_py.import_roms ./roms_atari > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines.common.atari_wrappers import make_atari, wrap_deepmind\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "import gym.wrappers\n",
        "# Creating a folder in Google Disk\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ],
      "metadata": {
        "id": "Gx_vTYyIILzh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "796b7275-a59f-4609-b6de-c956e8dfe1f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement de l'environnement GYM"
      ],
      "metadata": {
        "id": "q7CQdIZVYlxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_atari(\"BreakoutNoFrameskip-v4\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1L8msg1bKAyS",
        "outputId": "241e160f-b907-4da7-ab12-2cef7bba62d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "obs = np.array(env.reset())\n",
        "print(obs.shape)\n",
        "plt.title(\"environnement brut\")\n",
        "plt.imshow(obs)\n",
        "\n",
        "\n",
        "# Il ya ici une erreur d'attribut, il faut ouvrir le fichier erreur et modifier randint par integers et relancer le notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "M2J7M0qoIOsa",
        "outputId": "ed352db1-544d-49e2-8c12-0d0a5a6a6e55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-27889d287909>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"environnement brut\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines/common/atari_wrappers.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/stable_baselines/common/atari_wrappers.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mnoops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverride_num_noops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mnoops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrapped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoop_max\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mnoops\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'numpy.random._generator.Generator' object has no attribute 'randint'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "obs = np.array(env.reset())\n",
        "\n",
        "print(obs.shape)\n",
        "plt.title(\"agent observation (4 frames)\")\n",
        "plt.imshow(obs.transpose([0, 2, 1]).reshape([env.observation_space.shape[0], -1]))"
      ],
      "metadata": {
        "id": "ZNYZCfiTKxj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(env.action_space)\n",
        "print(env.get_action_meanings())\n",
        "     "
      ],
      "metadata": {
        "id": "y9ZAUJ8INszT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Déclaration des paramètres d'états\n",
        "env.reset()\n",
        "rewards = []\n",
        "while True:\n",
        "    obs, rew, done, info = env.step(env.action_space.sample())\n",
        "    rewards.append(rew)\n",
        "    if done:\n",
        "        break\n",
        "plt.plot(rewards)\n",
        "     "
      ],
      "metadata": {
        "id": "lt58rfNqNvyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Début du traitement avec création du CNN"
      ],
      "metadata": {
        "id": "zIP0ImI5PYAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "num_actions = 4\n",
        "def create_q_model():\n",
        "    # Réseau fortement inspiré du papier Attari sur RL\n",
        "    inputs = layers.Input(shape=(84, 84, 4,))\n",
        "\n",
        "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
        "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
        "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
        "    layer4 = layers.Flatten()(layer3)\n",
        "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
        "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=action)\n",
        "\n",
        "# Le réseau optimise la valeur de Q qui caractérise une fonction\n",
        "model = create_q_model()\n",
        "# on affiche le modèle\n",
        "model_target = create_q_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "8OvK4x--Nyzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gamma = 0.99   # Discount factor for past rewards\n",
        "\n",
        "# On définit nos paramètres epsilon d'exploration\n",
        "epsilon = 1.0  \n",
        "epsilon_max_1 = 1.0 \n",
        "epsilon_min_1 = 0.2  \n",
        "epsilon_max_2 = epsilon_min_1  \n",
        "epsilon_min_2 = 0.1\n",
        "epsilon_max_3 = epsilon_min_2  \n",
        "epsilon_min_3 = 0.02\n",
        "\n",
        "epsilon_interval_1 = (epsilon_max_1 - epsilon_min_1)  \n",
        "epsilon_interval_2 = (epsilon_max_2 - epsilon_min_2)  \n",
        "epsilon_interval_3 = (epsilon_max_3 - epsilon_min_3)  \n",
        "\n",
        "# Nombre maximum de frame\n",
        "epsilon_greedy_frames = 1000000.0\n",
        "\n",
        "# Numbre de frame dans lesquels on effectue une action aléatoire\n",
        "epsilon_random_frames = 50000\n",
        "\n",
        "# Mémoire de saturation=> dépend de la capacité de notre machine\n",
        "max_memory_length = 1900\n",
        "\n",
        "# Taille du batch \n",
        "#nombre max d'étapes par épisode sinon il se peut que le paddle ne touche \n",
        "#la boule. Cela permet d'éviter que l'entrainement tourne en boucle \n",
        "batch_size = 32  \n",
        "max_steps_per_episode = 10000\n",
        "\n",
        "# toutes les 20 actions, on entraine le modèle\n",
        "update_after_actions = 20\n",
        "\n",
        "# on actualise le\n",
        "update_target_network = 10000\n",
        "\n",
        "# Dans l'article de Deepmind, ils utilisent RMSProp, mais l'optimiseur Adam améliore le temps de formation.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=0.00025, clipnorm=1.0)\n",
        "\n",
        "# Using huber loss for stability\n",
        "loss_function = keras.losses.Huber()"
      ],
      "metadata": {
        "id": "NHnRL3NGN26u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Wqk19b3FO3eb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# On initialise les listes pour stocker les résultats\n",
        "action_history = []\n",
        "state_history = []\n",
        "state_next_history = []\n",
        "rewards_history = []\n",
        "done_history = []\n",
        "\n",
        "episode_reward_history = []\n",
        "running_reward = 0\n",
        "episode_count = 0\n",
        "frame_count = 0\n",
        "\n",
        "while True:  # On tourne dans la boucle tant que l'agent n'a pas résolu le problème\n",
        "    state = np.array(env.reset())\n",
        "    episode_reward = 0\n",
        "\n",
        "    for timestep in range(1, max_steps_per_episode):\n",
        "        \n",
        "        frame_count += 1\n",
        "\n",
        "        # On utilise la méthode d'exploration epsilon greedy\n",
        "        if frame_count < epsilon_random_frames or epsilon > np.random.rand(1)[0]:\n",
        "            # Take random action\n",
        "            action = np.random.choice(num_actions)\n",
        "        else:\n",
        "            # On prédit la valeur de Q \n",
        "            # A chaque état au sein de l'environnement\n",
        "            state_tensor = tf.convert_to_tensor(state)\n",
        "            state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "            action_probs = model(state_tensor, training=False)\n",
        "            # Take best action\n",
        "            action = tf.argmax(action_probs[0]).numpy()\n",
        "\n",
        "        # Decay probability of taking random action\n",
        "        if frame_count < epsilon_greedy_frames:\n",
        "          epsilon -= epsilon_interval_1 / epsilon_greedy_frames\n",
        "          epsilon = max(epsilon, epsilon_min_1)\n",
        "        \n",
        "        if frame_count > epsilon_greedy_frames and frame_count < 2 * epsilon_greedy_frames:\n",
        "          epsilon -= epsilon_interval_2 / epsilon_greedy_frames\n",
        "          epsilon = max(epsilon, epsilon_min_2)\n",
        "        \n",
        "        if frame_count > 2 * epsilon_greedy_frames:\n",
        "          epsilon -= epsilon_interval_3 / epsilon_greedy_frames\n",
        "          epsilon = max(epsilon, epsilon_min_3)\n",
        "          \n",
        "\n",
        "        # On applique les actions filtrées à l'environnement\n",
        "        state_next, reward, done, _ = env.step(action)\n",
        "        state_next = np.array(state_next)\n",
        "\n",
        "        episode_reward += reward\n",
        "\n",
        "        # On sauvegarde toute nos actions en mémoire \n",
        "        action_history.append(action)\n",
        "        state_history.append(state)\n",
        "        state_next_history.append(state_next)\n",
        "        done_history.append(done)\n",
        "        rewards_history.append(reward)\n",
        "        state = state_next\n",
        "\n",
        "        # Mise à jour toutes les 20 images et dès que la taille du batch est supérieure à 32.\n",
        "        if frame_count % update_after_actions == 0 and len(done_history) > batch_size:\n",
        "\n",
        "            # Obtenir les indices des échantillons\n",
        "            indices = np.random.choice(range(len(done_history)), size=batch_size)\n",
        "\n",
        "            # On recupère les informations dans les listes concernant les échantillon des états/action/reward\n",
        "            state_sample = np.array([state_history[i] for i in indices])\n",
        "            state_next_sample = np.array([state_next_history[i] for i in indices])\n",
        "            rewards_sample = [rewards_history[i] for i in indices]\n",
        "            action_sample = [action_history[i] for i in indices]\n",
        "            done_sample = tf.convert_to_tensor(\n",
        "                [float(done_history[i]) for i in indices]\n",
        "            )\n",
        "\n",
        "            #Construire les valeurs Q mises à jour pour les états futurs échantillonnés.\n",
        "            #Utiliser le modèle cible pour la stabilité\n",
        "            future_rewards = model_target.predict(state_next_sample)\n",
        "            # Q value = reward + discount factor * expected future reward\n",
        "            updated_q_values = rewards_sample + gamma * tf.reduce_max(\n",
        "                future_rewards, axis=1\n",
        "            )\n",
        "\n",
        "            # Si c'est la dernière image, la dernière valeur sera -1\n",
        "            updated_q_values = updated_q_values * (1 - done_sample) - done_sample\n",
        "\n",
        "            #on crée un masque pour que nous calculer la loss que sur les valeurs Q mises à jour.\n",
        "            masks = tf.one_hot(action_sample, num_actions)\n",
        "\n",
        "            with tf.GradientTape() as tape:\n",
        "                # On entraine le modèle sur les états et les Q values actualisées\n",
        "                q_values = model(state_sample)\n",
        "\n",
        "                # on applique les masques aux valeurs Q pour obtenir la valeur Q de l'action entreprise.\n",
        "                q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\n",
        "                # on recalcule une loss entre l'ancienne et la nouvelle q value\n",
        "                loss = loss_function(updated_q_values, q_action)\n",
        "\n",
        "            # Backpropagation\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "        if frame_count % update_target_network == 0:\n",
        "            #on actualise avec les ouveaux poids\n",
        "            model_target.set_weights(model.get_weights())\n",
        "            # on décrit lors des étapes\n",
        "            template = \"running reward: {:.2f} at episode {}, frame count {}, epsilon {:.3f}, loss {:.5f}\"\n",
        "            print(template.format(running_reward, episode_count, frame_count, epsilon, loss))\n",
        "\n",
        "        #on limite l'historique des rewards et des états\n",
        "        if len(rewards_history) > max_memory_length:\n",
        "            del rewards_history[:1]\n",
        "            del state_history[:1]\n",
        "            del state_next_history[:1]\n",
        "            del action_history[:1]\n",
        "            del done_history[:1]\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    episode_reward_history.append(episode_reward)\n",
        "    if len(episode_reward_history) > 100:\n",
        "        del episode_reward_history[:1]\n",
        "    running_reward = np.mean(episode_reward_history)\n",
        "\n",
        "    episode_count += 1\n",
        "\n",
        "    if running_reward > 5:  # On considère ici que l'agent a resolu le jeu dès que sa reward dépasse 5\n",
        "        print(\"Solved at episode {}!\".format(episode_count))\n",
        "        break"
      ],
      "metadata": {
        "id": "RexsMw4XN8lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde du modèle\n",
        "#model_name = 'breakout_model_1'\n",
        "path = F\"/content/gdrive/MyDrive/3A/Projet Parcours/models/{model_name}\" \n",
        "model.save(path)\n",
        "\n",
        "# Chargement du modèle\n",
        "model = tf.keras.models.load_model(path)\n",
        "\n",
        "#model_name = 'breakout'\n",
        "#path = F\"/content/gdrive/MyDrive/{model_name}\" \n",
        "\n",
        "#model = tf.keras.models.load_model(path)\n"
      ],
      "metadata": {
        "id": "ikmuWrFLm3sp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_env():\n",
        "  env = make_atari(\"BreakoutNoFrameskip-v4\")\n",
        "  env = wrap_deepmind(env, frame_stack=True, scale=True)\n",
        "  return env\n",
        "     "
      ],
      "metadata": {
        "id": "1QAbFoCXdarR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gym.wrappers.monitoring.video_recorder import VideoRecorder"
      ],
      "metadata": {
        "id": "7RXQwjt1dsUc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# On paramètre l'affichage et le recording de notre breakout sur la base du modèle qu'on vient d'entrainer\n",
        "\n",
        "env = make_env()\n",
        "env = gym.wrappers.RecordVideo(env, \"/content/gdrive/MyDrive/3A/Projet Parcours/models/breakout_model_1/vid1\")\n",
        "\n",
        "observation = env.reset()\n",
        "info = 0\n",
        "reward_window = []\n",
        "reward_signal_history = []\n",
        "epsilon_history = []\n",
        "\n",
        "hits = []\n",
        "bltd = 3 #On indique le nombre de brique l'on veut voir détruites\n",
        "\n",
        "for i_episode in range(1):\n",
        "    reward_window=[] \n",
        "    epsilon = 0  \n",
        "    for t in range(100):# on parcourt le nombre de frames correspondantes \n",
        "        \n",
        "        if epsilon > np.random.rand(1)[0]:\n",
        "          action = np.random.choice(num_actions)\n",
        "        else:\n",
        "          state_tensor = tf.convert_to_tensor(observation)\n",
        "          state_tensor = tf.expand_dims(state_tensor, 0)\n",
        "          action_probs = model(state_tensor, training=False)\n",
        "          action = tf.argmax(action_probs[0]).numpy()\n",
        "        \n",
        "        observation, reward, done, info = env.step(action)\n",
        "        hits.append(reward)\n",
        "        reward_window.append(reward)\n",
        "        if len(reward_window) > 200:\n",
        "          del reward_window[:1] \n",
        "        if len(reward_window) == 200 and np.sum(reward_window) == 0:\n",
        "          epsilon = 0.01\n",
        "        else:\n",
        "          epsilon = 0.0001\n",
        "\n",
        "        epsilon_history.append(epsilon)\n",
        "        reward_signal_history.append(reward)\n",
        "\n",
        "        \n",
        "        if done:\n",
        "            print(\"Lost one life after {} timesteps\".format(t+1))\n",
        "            print(info)\n",
        "            # On plot l'évolution d'epsilon et des rewards\n",
        "            fig,ax=plt.subplots(figsize=(20,3))\n",
        "            #plt.clf()\n",
        "            ax.plot(epsilon_history, color=\"red\")\n",
        "            ax.set_ylabel(\"epsilon\",color=\"red\",fontsize=14)\n",
        "            ax2=ax.twinx()\n",
        "            ax2.plot(reward_signal_history,color=\"blue\")\n",
        "            ax2.set_ylabel(\"reward_signal\",color=\"blue\",fontsize=14)\n",
        "            plt.show()\n",
        "\n",
        "            epsilon_history = []\n",
        "            reward_signal_history = []\n",
        "            \n",
        "            bltd = bltd-np.sum(hits)\n",
        "            hits = []\n",
        "            print(\"Bricks left to destroy \", bltd)\n",
        "            if info['ale.lives'] == 0:\n",
        "              break\n",
        "\n",
        "            env.reset()\n",
        "env.close()"
      ],
      "metadata": {
        "id": "aNmAq06Pdeyj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}